{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a4fe4f4-1103-4155-841c-2fd31dc88cff",
    "_uuid": "81c7becc-0edb-4fec-9f61-0fd0c78f951e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Adaptive Learning Engine for Persuasion Training\n",
    "This notebook implements an end-to-end adaptive learning pipeline that analyzes persuasion effectiveness in gaming recommendations. By combining large language models (LLMs) for feature extraction, contextual bandits for intelligent decision-making, and personalized coaching generation, we create a data-driven system that helps users improve their persuasion skills over time.\n",
    "\n",
    "## Overview\n",
    "1. extracts LLM-based features from every session transcript,\n",
    "2. implements an adaptive policy to choose next-step focus,\n",
    "3. produces adaptive coaching for the next session (focus skill + coaching card + scenario stub).\n",
    "4. Results Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5258ecea-8014-48ec-b593-c29ae665d234",
    "_uuid": "63699789-f956-4edd-88c8-9939e311316a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 1. Setup and Dependencies\n",
    "First, let's install and import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a3835058-3bab-41c7-85c7-44cdabe90d0b",
    "_uuid": "eb609969-d0f0-4128-b13e-6771a6c019e6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:18:27.706403Z",
     "iopub.status.busy": "2025-11-08T19:18:27.706241Z",
     "iopub.status.idle": "2025-11-08T19:19:49.851824Z",
     "shell.execute_reply": "2025-11-08T19:19:49.850875Z",
     "shell.execute_reply.started": "2025-11-08T19:18:27.706382Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch numpy pandas plotly scikit-learn python-dotenv diskcache tqdm huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ea2177e-6cb7-4e71-982a-2c8ebcb0cd60",
    "_uuid": "269ac203-3d17-479c-8848-35eef2ee7f87",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:19:49.854172Z",
     "iopub.status.busy": "2025-11-08T19:19:49.853564Z",
     "iopub.status.idle": "2025-11-08T19:20:26.237118Z",
     "shell.execute_reply": "2025-11-08T19:20:26.236531Z",
     "shell.execute_reply.started": "2025-11-08T19:19:49.854149Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from transformers import pipeline\n",
    "import diskcache\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06534b09-254c-4915-9293-90bf01e4120a",
    "_uuid": "d3c3f721-7d05-4467-92d6-798f1b04c4c7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 2. Feature Extractor Implementation\n",
    "We'll implement our PersuasionFeatureExtractor class that will compute 7 different features:\n",
    "1. Objection Handling\n",
    "2. Question Engagement\n",
    "3. CTA Clarity\n",
    "4. Empathy Score\n",
    "5. Collaborative Language\n",
    "6. Social Proof\n",
    "7. Enthusiasm Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5c00b60-ce71-4667-a6c9-8ea9f5c3c0aa",
    "_uuid": "b3baed80-7007-4d43-97df-6febf799d11b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:26.240604Z",
     "iopub.status.busy": "2025-11-08T19:20:26.240343Z",
     "iopub.status.idle": "2025-11-08T19:20:26.252204Z",
     "shell.execute_reply": "2025-11-08T19:20:26.251476Z",
     "shell.execute_reply.started": "2025-11-08T19:20:26.240581Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PersuasionFeatureExtractor:\n",
    "    def __init__(self, model_name: str = \"facebook/bart-large-mnli\"):\n",
    "        \"\"\"Initialize the feature extractor with the specified model.\"\"\"\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "        self.cache = diskcache.Cache('./.cache/feature_extraction')\n",
    "        \n",
    "    def compute_objection_handling(self, text: str) -> float:\n",
    "        \"\"\"Measure how well objections are acknowledged and addressed.\"\"\"\n",
    "        cache_key = f\"objection_handling_{hash(text)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        labels = [\n",
    "            \"acknowledges and addresses concerns\",\n",
    "            \"ignores or dismisses concerns\"\n",
    "        ]\n",
    "        result = self.classifier(text, labels, multi_label=False)\n",
    "        score = result['scores'][0]\n",
    "        self.cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def compute_question_ratio(self, turns: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"Calculate the ratio of turns containing questions.\"\"\"\n",
    "        question_count = sum(1 for turn in turns if '?' in turn['data'])\n",
    "        return min(1.0, question_count / len(turns))\n",
    "    \n",
    "    def compute_cta_clarity(self, text: str) -> float:\n",
    "        \"\"\"Measure the clarity and specificity of the call to action.\"\"\"\n",
    "        cache_key = f\"cta_clarity_{hash(text)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        labels = [\n",
    "            \"contains specific invitation with clear next steps\",\n",
    "            \"vague or missing call to action\"\n",
    "        ]\n",
    "        result = self.classifier(text, labels, multi_label=False)\n",
    "        score = result['scores'][0]\n",
    "        self.cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def compute_empathy_score(self, text: str) -> float:\n",
    "        \"\"\"Measure the level of empathy and emotional understanding.\"\"\"\n",
    "        cache_key = f\"empathy_{hash(text)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        labels = [\n",
    "            \"shows understanding and emotional awareness\",\n",
    "            \"lacks empathy or emotional connection\"\n",
    "        ]\n",
    "        result = self.classifier(text, labels, multi_label=False)\n",
    "        score = result['scores'][0]\n",
    "        self.cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def compute_collaborative_score(self, text: str) -> float:\n",
    "        \"\"\"Measure the use of collaborative vs. pushy language.\"\"\"\n",
    "        cache_key = f\"collaborative_{hash(text)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        labels = [\n",
    "            \"uses collaborative and inclusive language\",\n",
    "            \"uses pushy or aggressive language\"\n",
    "        ]\n",
    "        result = self.classifier(text, labels, multi_label=False)\n",
    "        score = result['scores'][0]\n",
    "        self.cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def compute_social_proof(self, text: str) -> float:\n",
    "        \"\"\"Measure references to community and shared experiences.\"\"\"\n",
    "        cache_key = f\"social_proof_{hash(text)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        labels = [\n",
    "            \"references community or shared experiences\",\n",
    "            \"lacks social context or community references\"\n",
    "        ]\n",
    "        result = self.classifier(text, labels, multi_label=False)\n",
    "        score = result['scores'][0]\n",
    "        self.cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def compute_enthusiasm(self, text: str) -> float:\n",
    "        \"\"\"Measure genuine excitement and positive energy.\"\"\"\n",
    "        cache_key = f\"enthusiasm_{hash(text)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "            \n",
    "        labels = [\n",
    "            \"shows genuine excitement and enthusiasm\",\n",
    "            \"lacks energy or excitement\"\n",
    "        ]\n",
    "        result = self.classifier(text, labels, multi_label=False)\n",
    "        score = result['scores'][0]\n",
    "        self.cache[cache_key] = score\n",
    "        return score\n",
    "    \n",
    "    def extract_features(self, transcript: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"Extract all features from a conversation transcript.\"\"\"\n",
    "        # Combine all user turns into one text for analysis\n",
    "        user_text = \" \".join(turn['data'] for turn in transcript if turn['type'] == 'user')\n",
    "        \n",
    "        features = {\n",
    "            'objection_handling': self.compute_objection_handling(user_text),\n",
    "            'question_ratio': self.compute_question_ratio(transcript),\n",
    "            'cta_clarity': self.compute_cta_clarity(user_text),\n",
    "            'empathy_score': self.compute_empathy_score(user_text),\n",
    "            'collaborative_score': self.compute_collaborative_score(user_text),\n",
    "            'social_proof': self.compute_social_proof(user_text),\n",
    "            'enthusiasm': self.compute_enthusiasm(user_text)\n",
    "        }\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3955ee7b-b760-4efd-8f5f-0440513538d0",
    "_uuid": "1d28082f-b2e0-4ee4-b60f-e1e29d20bbe9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 3. Data Loading and Processing\n",
    "Now let's load our conversation data and process it using our feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d80fe2f3-17b6-4d10-b6ac-2a6845b34821",
    "_uuid": "33012ee0-fccf-4609-9d7e-e194627a1017",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:26.254733Z",
     "iopub.status.busy": "2025-11-08T19:20:26.254474Z",
     "iopub.status.idle": "2025-11-08T19:20:47.635430Z",
     "shell.execute_reply": "2025-11-08T19:20:47.634776Z",
     "shell.execute_reply.started": "2025-11-08T19:20:26.254708Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/input/sanitized-sample/sanitized-sample.json\"\n",
    "\n",
    "# Load the conversation data\n",
    "with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")\n",
    "\n",
    "# Initialize our feature extractor\n",
    "extractor = PersuasionFeatureExtractor()\n",
    "\n",
    "# Process all conversations with a progress bar\n",
    "results = []\n",
    "for conv in tqdm(conversations, desc=\"Extracting features\"):\n",
    "    features = extractor.extract_features(conv['transcript'])\n",
    "    features['conversation_id'] = conv['id']\n",
    "    features['duration'] = (\n",
    "        pd.to_datetime(conv['end_time']) -\n",
    "        pd.to_datetime(conv['start_time'])\n",
    "    ).total_seconds()\n",
    "    results.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(results)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c28b69e0-84bf-498e-82ca-f45df0ad8f50",
    "_uuid": "896a2832-8a0b-438b-8575-94b8b080ef8b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 4. Feature Analysis and Visualization\n",
    "Let's create some visualizations to understand our extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da483909-f46c-4c33-8478-2232a77e7e6c",
    "_uuid": "00730603-47f6-4aeb-a92c-0f2074917da0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:47.636573Z",
     "iopub.status.busy": "2025-11-08T19:20:47.636241Z",
     "iopub.status.idle": "2025-11-08T19:20:49.095627Z",
     "shell.execute_reply": "2025-11-08T19:20:49.095011Z",
     "shell.execute_reply.started": "2025-11-08T19:20:47.636551Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "fig = go.Figure()\n",
    "for feature in features_df.columns:\n",
    "    if feature not in ['conversation_id', 'duration']:\n",
    "        fig.add_trace(go.Box(y=features_df[feature], name=feature))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Persuasion Features\",\n",
    "    yaxis_title=\"Score\",\n",
    "    showlegend=False,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d0fa590-a159-4006-9c09-85f998ae2f8c",
    "_uuid": "2b3be923-c76e-40fb-8639-40867afe0f1f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:49.096485Z",
     "iopub.status.busy": "2025-11-08T19:20:49.096300Z",
     "iopub.status.idle": "2025-11-08T19:20:50.257653Z",
     "shell.execute_reply": "2025-11-08T19:20:50.256920Z",
     "shell.execute_reply.started": "2025-11-08T19:20:49.096470Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "feature_cols = [col for col in features_df.columns if col not in ['conversation_id', 'duration']]\n",
    "correlation_matrix = features_df[feature_cols].corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix,\n",
    "    x=feature_cols,\n",
    "    y=feature_cols,\n",
    "    colorscale='RdBu',\n",
    "    zmin=-1,\n",
    "    zmax=1\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Correlation Heatmap\",\n",
    "    height=600,\n",
    "    width=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a139ef61-f0be-46f8-849b-1e0de7df0c66",
    "_uuid": "546c94b4-9c84-4715-8c87-8245c90c1396",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 5. Export Results\n",
    "Finally, let's save our extracted features for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83e6b046-38b0-4b4c-a555-75ce41f7249a",
    "_uuid": "66def836-56b5-4c6a-89bb-ba2907b70b55",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.258745Z",
     "iopub.status.busy": "2025-11-08T19:20:50.258523Z",
     "iopub.status.idle": "2025-11-08T19:20:50.286153Z",
     "shell.execute_reply": "2025-11-08T19:20:50.285588Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.258728Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'data/extracted_features.csv'\n",
    "features_df.to_csv(output_path, index=False)\n",
    "print(f\"Features saved to {output_path}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(features_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af04a675-d69b-4736-9a60-a83b663384ca",
    "_uuid": "48bd4576-5799-4564-8022-4208aab52921",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Next Steps\n",
    "1. Use these features to train a model that predicts overall persuasion effectiveness\n",
    "2. Implement an adaptive policy for choosing which skills to focus on\n",
    "3. Generate personalized coaching recommendations based on the feature analysis\n",
    "\n",
    "The extracted features provide a rich quantitative basis for analyzing persuasion effectiveness in gaming recommendations. The next notebook will focus on using these features to build an adaptive learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8bfec1eb-4e8e-498e-b44e-17d10d5485ad",
    "_uuid": "aad6e4bb-49e5-4a01-acba-c153f24709a0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Part B: Adaptive Policy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "81017435-df90-48d7-b90b-d67228116429",
    "_uuid": "7f57ff54-bb8b-46cc-8776-477a17ca305b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 1. Policy Implementation - LinUCB Contextual Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "922eada3-2b12-4ab0-a97b-bf6b7fdddf84",
    "_uuid": "dbf8fff3-e11f-4ba0-a74a-229a036dbf7d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.287241Z",
     "iopub.status.busy": "2025-11-08T19:20:50.286977Z",
     "iopub.status.idle": "2025-11-08T19:20:50.300531Z",
     "shell.execute_reply": "2025-11-08T19:20:50.299662Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.287218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ActionHistory:\n",
    "    \"\"\"Track action history for safety constraints.\"\"\"\n",
    "    action: str\n",
    "    count: int = 1\n",
    "\n",
    "class LinUCBPolicy:\n",
    "    \"\"\"\n",
    "    Linear Upper Confidence Bound policy for skill focus selection.\n",
    "    \n",
    "    Action space: {clarity, active_listening, call_to_action, friendliness}\n",
    "    Maps to rubric criteria:\n",
    "    - clarity -> Clarity & Enthusiasm\n",
    "    - active_listening -> Active Listening & Objection Handling\n",
    "    - call_to_action -> Effective Call to Action\n",
    "    - friendliness -> Friendliness & Respectful Tone\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features: int, alpha: float = 1.0, random_seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize LinUCB policy.\n",
    "        \n",
    "        Args:\n",
    "            n_features: Dimension of context vector\n",
    "            alpha: Exploration parameter (higher = more exploration)\n",
    "            random_seed: For reproducibility\n",
    "        \"\"\"\n",
    "        self.actions = ['clarity', 'active_listening', 'call_to_action', 'friendliness']\n",
    "        self.action_to_rubric = {\n",
    "            'clarity': 'Clarity & Enthusiasm',\n",
    "            'active_listening': 'Active Listening & Objection Handling',\n",
    "            'call_to_action': 'Effective Call to Action',\n",
    "            'friendliness': 'Friendliness & Respectful Tone'\n",
    "        }\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        self.rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        # Initialize parameters for each action\n",
    "        # A: design matrix (regularization + outer products)\n",
    "        # b: response vector (weighted rewards)\n",
    "        self.A = {action: np.eye(n_features) for action in self.actions}\n",
    "        self.b = {action: np.zeros(n_features) for action in self.actions}\n",
    "        \n",
    "        # Action history for safety constraint\n",
    "        self.action_history = []\n",
    "        self.max_consecutive = 3\n",
    "        \n",
    "    def _get_context_vector(self, features: Dict[str, float], \n",
    "                           rubric_scores: Dict[str, float],\n",
    "                           session_idx: int,\n",
    "                           total_sessions: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build context vector from features and metadata.\n",
    "        \n",
    "        Context includes:\n",
    "        - Current rubric scores (4 features)\n",
    "        - LLM features (7 features)\n",
    "        - Session progress (1 feature)\n",
    "        - Improvement rate (1 feature, set to 0 for first session)\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # Rubric scores (normalized to [0,1])\n",
    "        rubric_order = ['Clarity & Enthusiasm', \n",
    "                       'Active Listening & Objection Handling',\n",
    "                       'Effective Call to Action', \n",
    "                       'Friendliness & Respectful Tone']\n",
    "        context.extend([rubric_scores.get(r, 0) / 100.0 for r in rubric_order])\n",
    "        \n",
    "        # LLM features (already in [0,1])\n",
    "        llm_features = ['objection_handling', 'question_ratio', 'cta_clarity',\n",
    "                       'empathy_score', 'collaborative_score', 'social_proof', \n",
    "                       'enthusiasm']\n",
    "        context.extend([features.get(f, 0) for f in llm_features])\n",
    "        \n",
    "        # Session progress\n",
    "        progress = session_idx / max(1, total_sessions - 1)\n",
    "        context.append(progress)\n",
    "        \n",
    "        # Improvement rate (placeholder for first session)\n",
    "        context.append(0.0)\n",
    "        \n",
    "        return np.array(context)\n",
    "    \n",
    "    def _check_safety_constraint(self, action: str) -> bool:\n",
    "        \"\"\"Check if selecting this action violates safety constraint.\"\"\"\n",
    "        if len(self.action_history) < self.max_consecutive:\n",
    "            return True\n",
    "        \n",
    "        recent_actions = self.action_history[-self.max_consecutive:]\n",
    "        return not all(a == action for a in recent_actions)\n",
    "    \n",
    "    def select_action(self, context: np.ndarray, enforce_safety: bool = True) -> Tuple[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Select action using LinUCB algorithm with safety constraints.\n",
    "        \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "            ucb_scores: UCB scores for all actions (for analysis)\n",
    "        \"\"\"\n",
    "        ucb_scores = {}\n",
    "        \n",
    "        for action in self.actions:\n",
    "            # Compute theta (parameter estimate)\n",
    "            A_inv = np.linalg.inv(self.A[action])\n",
    "            theta = A_inv @ self.b[action]\n",
    "            \n",
    "            # Compute UCB: predicted reward + uncertainty bonus\n",
    "            prediction = theta @ context\n",
    "            uncertainty = self.alpha * np.sqrt(context @ A_inv @ context)\n",
    "            ucb = prediction + uncertainty\n",
    "            \n",
    "            # Apply safety constraint\n",
    "            if enforce_safety and not self._check_safety_constraint(action):\n",
    "                ucb -= 1000  # Large penalty to avoid selection\n",
    "            \n",
    "            ucb_scores[action] = ucb\n",
    "        \n",
    "        # Select action with highest UCB\n",
    "        selected_action = max(ucb_scores, key=ucb_scores.get)\n",
    "        self.action_history.append(selected_action)\n",
    "        \n",
    "        return selected_action, ucb_scores\n",
    "    \n",
    "    def update(self, action: str, context: np.ndarray, reward: float):\n",
    "        \"\"\"\n",
    "        Update model parameters after observing reward.\n",
    "        \n",
    "        Args:\n",
    "            action: Action taken\n",
    "            context: Context vector when action was taken\n",
    "            reward: Observed reward\n",
    "        \"\"\"\n",
    "        self.A[action] += np.outer(context, context)\n",
    "        self.b[action] += reward * context\n",
    "    \n",
    "    def compute_reward(self, current_rubric: Dict[str, float],\n",
    "                      previous_rubric: Optional[Dict[str, float]],\n",
    "                      action: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute reward: 0.6 * Î”(skill_focus) + 0.4 * Î”(overall)\n",
    "        \n",
    "        Args:\n",
    "            current_rubric: Current session rubric scores\n",
    "            previous_rubric: Previous session rubric scores (None for first session)\n",
    "            action: Action that was taken\n",
    "        \"\"\"\n",
    "        if previous_rubric is None:\n",
    "            return 0.0  # No reward for first session\n",
    "        \n",
    "        # Map action to corresponding rubric criterion\n",
    "        skill_name = self.action_to_rubric[action]\n",
    "        \n",
    "        # Compute improvements (normalized by 100)\n",
    "        delta_skill = (current_rubric.get(skill_name, 0) - \n",
    "                      previous_rubric.get(skill_name, 0)) / 100.0\n",
    "        delta_overall = (current_rubric.get('Overall', 0) - \n",
    "                        previous_rubric.get('Overall', 0)) / 100.0\n",
    "        \n",
    "        reward = 0.6 * delta_skill + 0.4 * delta_overall\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "575f1b45-a24f-4665-b763-8ae31db2f1bd",
    "_uuid": "1b5eb30b-79ee-4e10-8978-71f4c8be9f5f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2. Load Rubric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2df4e94d-d66c-4679-80d6-7cae067de57a",
    "_uuid": "cd771d83-bb1d-4c58-8621-6e5db9c13c97",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.301733Z",
     "iopub.status.busy": "2025-11-08T19:20:50.301364Z",
     "iopub.status.idle": "2025-11-08T19:20:50.338710Z",
     "shell.execute_reply": "2025-11-08T19:20:50.338176Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.301709Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load conversations with rubric scores\n",
    "import json\n",
    "file_path = \"/kaggle/input/sanitized-sample/sanitized-sample.json\"\n",
    "with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "# Extract rubric scores for each conversation\n",
    "rubric_data = []\n",
    "\n",
    "for conv in conversations:\n",
    "    assessment = conv.get(\"assessment_data\", {})\n",
    "    overall_score = assessment.get(\"overall\", {}).get(\"score\", None)\n",
    "    criteria = assessment.get(\"criteria\", {})\n",
    "\n",
    "    rubric_scores = {\n",
    "        \"conversation_id\": conv.get(\"id\", None),\n",
    "        \"Overall\": overall_score\n",
    "    }\n",
    "\n",
    "    # Add each criterion\n",
    "    for name, details in criteria.items():\n",
    "        rubric_scores[name] = details.get(\"score\", None)\n",
    "\n",
    "    rubric_data.append(rubric_scores)\n",
    "\n",
    "rubric_df = pd.DataFrame(rubric_data)\n",
    "print(\"Rubric Scores:\")\n",
    "print(rubric_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8998507-2880-4289-be7e-32ada34b8ab5",
    "_uuid": "e32c3323-7538-4925-ae27-9ae5bf6b141e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 3. Merge Features with Rubric Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75fe3068-a4a1-4170-b06b-44dd38a582a5",
    "_uuid": "2039069b-3580-4e19-860e-c96b8c4b1ad3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.339793Z",
     "iopub.status.busy": "2025-11-08T19:20:50.339444Z",
     "iopub.status.idle": "2025-11-08T19:20:50.343870Z",
     "shell.execute_reply": "2025-11-08T19:20:50.343108Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.339751Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(globals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "08999533-916b-4124-a275-7cdb7c589b47",
    "_uuid": "f4b6faba-27fd-4442-9a99-a252c8a2ebbb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.345187Z",
     "iopub.status.busy": "2025-11-08T19:20:50.344880Z",
     "iopub.status.idle": "2025-11-08T19:20:50.368376Z",
     "shell.execute_reply": "2025-11-08T19:20:50.367735Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.345162Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge LLM features with rubric scores\n",
    "full_df = features_df.merge(rubric_df, on='conversation_id', how='inner')\n",
    "\n",
    "# Sort by conversation start time (assuming conversation order = session order)\n",
    "# For this demo, we'll use the original order as session sequence\n",
    "full_df = full_df.reset_index(drop=True)\n",
    "full_df['session_idx'] = range(len(full_df))\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {full_df.shape}\")\n",
    "print(full_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9fe00cd2-03a8-408e-95da-5c4283bbf3ba",
    "_uuid": "1cab15b4-4bc7-4c80-8a34-6d9866895599",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 4. Run LinUCB Policy (Offline Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "644fc619-8e5d-4d11-9124-2f0be2bfd70b",
    "_uuid": "14780610-a7b6-431e-86f8-650e63d800eb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.369293Z",
     "iopub.status.busy": "2025-11-08T19:20:50.369012Z",
     "iopub.status.idle": "2025-11-08T19:20:50.385814Z",
     "shell.execute_reply": "2025-11-08T19:20:50.385053Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.369277Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(full_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc3bb089-e2b4-4934-89b1-194003a6f1c9",
    "_uuid": "31537c51-33ee-4694-b8e1-238bb84f8c0e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.388983Z",
     "iopub.status.busy": "2025-11-08T19:20:50.388726Z",
     "iopub.status.idle": "2025-11-08T19:20:50.505665Z",
     "shell.execute_reply": "2025-11-08T19:20:50.504962Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.388961Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize policy\n",
    "n_features = 13  # 4 rubric + 7 LLM + 1 progress + 1 improvement_rate\n",
    "policy = LinUCBPolicy(n_features=n_features, alpha=1.0, random_seed=42)\n",
    "\n",
    "# Track results\n",
    "policy_results = []\n",
    "\n",
    "for idx in range(len(full_df)):\n",
    "    current_row = full_df.iloc[idx]\n",
    "    \n",
    "    # Build context\n",
    "    features = current_row[['objection_handling', 'question_ratio', 'cta_clarity',\n",
    "                            'empathy_score', 'collaborative_score', 'social_proof', \n",
    "                            'enthusiasm']].to_dict()\n",
    "    \n",
    "    # Use correct lowercase column names\n",
    "    rubric_scores = current_row[['clarity and enthusiasm in pitch', \n",
    "                                 'active listening and objection handling',\n",
    "                                 'effective call to action', \n",
    "                                 'friendliness and respectful tone',\n",
    "                                 'Overall']].to_dict()\n",
    "    \n",
    "    # Update improvement rate if not first session\n",
    "    if idx > 0:\n",
    "        prev_overall = full_df.iloc[idx-1]['Overall']\n",
    "        improvement_rate = (rubric_scores['Overall'] - prev_overall) / 100.0\n",
    "    else:\n",
    "        improvement_rate = 0.0\n",
    "    \n",
    "    # Build context vector (depends on your LinUCBPolicy implementation)\n",
    "    context = policy._get_context_vector(features, rubric_scores, idx, len(full_df))\n",
    "    context[-1] = improvement_rate  # Update improvement rate\n",
    "    \n",
    "    # Select action\n",
    "    action, ucb_scores = policy.select_action(context, enforce_safety=True)\n",
    "    \n",
    "    # Compute reward (using previous session for comparison)\n",
    "    if idx > 0:\n",
    "        prev_rubric = full_df.iloc[idx-1][['clarity and enthusiasm in pitch', \n",
    "                                           'active listening and objection handling',\n",
    "                                           'effective call to action', \n",
    "                                           'friendliness and respectful tone',\n",
    "                                           'Overall']].to_dict()\n",
    "        reward = policy.compute_reward(rubric_scores, prev_rubric, action)\n",
    "        \n",
    "        # Update policy with reward\n",
    "        policy.update(action, context, reward)\n",
    "    else:\n",
    "        reward = 0.0\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'session_idx': idx,\n",
    "        'conversation_id': current_row['conversation_id'],\n",
    "        'selected_action': action,\n",
    "        'reward': reward,\n",
    "        'overall_score': rubric_scores['Overall'],\n",
    "        'improvement_rate': improvement_rate\n",
    "    }\n",
    "    result.update({f'ucb_{k}': v for k, v in ucb_scores.items()})\n",
    "    policy_results.append(result)\n",
    "\n",
    "policy_df = pd.DataFrame(policy_results)\n",
    "print(\"\\nPolicy Results:\")\n",
    "print(policy_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39732c05-9930-4d3a-82c3-bb923ff96b28",
    "_uuid": "85cbdec1-bce5-472b-9190-4a4a42f70539",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 5. Visualize Policy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b593ac13-553b-48e8-9619-37102cbad5b8",
    "_uuid": "7de82895-e1b3-484c-80f8-563b8b362fd5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:50.506657Z",
     "iopub.status.busy": "2025-11-08T19:20:50.506361Z",
     "iopub.status.idle": "2025-11-08T19:20:52.778513Z",
     "shell.execute_reply": "2025-11-08T19:20:52.777925Z",
     "shell.execute_reply.started": "2025-11-08T19:20:50.506642Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Action distribution\n",
    "fig = px.histogram(policy_df, x='selected_action', \n",
    "                   title='Distribution of Selected Actions',\n",
    "                   labels={'selected_action': 'Action', 'count': 'Frequency'})\n",
    "fig.show()\n",
    "\n",
    "# Reward over time\n",
    "fig = px.line(policy_df, x='session_idx', y='reward',\n",
    "              title='Reward Over Sessions',\n",
    "              labels={'session_idx': 'Session', 'reward': 'Reward'})\n",
    "fig.show()\n",
    "\n",
    "# Overall score progression\n",
    "fig = px.line(policy_df, x='session_idx', y='overall_score',\n",
    "              title='Overall Score Progression',\n",
    "              labels={'session_idx': 'Session', 'overall_score': 'Overall Score'})\n",
    "fig.show()\n",
    "\n",
    "# UCB scores over time (for first action as example)\n",
    "ucb_cols = [col for col in policy_df.columns if col.startswith('ucb_')]\n",
    "fig = go.Figure()\n",
    "for col in ucb_cols:\n",
    "    action_name = col.replace('ucb_', '')\n",
    "    fig.add_trace(go.Scatter(x=policy_df['session_idx'], y=policy_df[col],\n",
    "                            mode='lines', name=action_name))\n",
    "fig.update_layout(title='UCB Scores Over Sessions',\n",
    "                 xaxis_title='Session',\n",
    "                 yaxis_title='UCB Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1c48cd8-9062-4b4a-a95f-b501a909cbd0",
    "_uuid": "74fc2181-28d4-4507-a106-9c2d997cd4e3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 6. Safety Constraint Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "37085639-ce54-4761-bd1d-e45df3f95ac5",
    "_uuid": "76c11123-f9d1-4208-9387-aa3b0ad54818",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.779493Z",
     "iopub.status.busy": "2025-11-08T19:20:52.779232Z",
     "iopub.status.idle": "2025-11-08T19:20:52.785322Z",
     "shell.execute_reply": "2025-11-08T19:20:52.784501Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.779468Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check consecutive action streaks\n",
    "def find_consecutive_streaks(actions):\n",
    "    \"\"\"Find maximum consecutive occurrences of each action.\"\"\"\n",
    "    max_streaks = {}\n",
    "    for action in set(actions):\n",
    "        max_streak = 0\n",
    "        current_streak = 0\n",
    "        for a in actions:\n",
    "            if a == action:\n",
    "                current_streak += 1\n",
    "                max_streak = max(max_streak, current_streak)\n",
    "            else:\n",
    "                current_streak = 0\n",
    "        max_streaks[action] = max_streak\n",
    "    return max_streaks\n",
    "\n",
    "streaks = find_consecutive_streaks(policy_df['selected_action'].tolist())\n",
    "print(\"\\nSafety Constraint Analysis:\")\n",
    "print(f\"Maximum consecutive selections per action:\")\n",
    "for action, max_streak in streaks.items():\n",
    "    status = \"âœ“ SAFE\" if max_streak <= 3 else \"âœ— VIOLATED\"\n",
    "    print(f\"  {action}: {max_streak} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f586a22-d96b-42c6-afb4-5049a0b002ae",
    "_uuid": "c531164d-f1b1-4d51-bdb1-5a0a60fb3403",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "572cb8ec-efda-44a3-b51f-4c7dcf8e826a",
    "_uuid": "43b90ce2-1e85-4f65-88d9-27f44cb0715f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.786460Z",
     "iopub.status.busy": "2025-11-08T19:20:52.786182Z",
     "iopub.status.idle": "2025-11-08T19:20:52.816249Z",
     "shell.execute_reply": "2025-11-08T19:20:52.815541Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.786439Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save policy results\n",
    "policy_df.to_csv('data/policy_results.csv', index=False)\n",
    "print(\"\\nPolicy results saved to 'data/policy_results.csv'\")\n",
    "\n",
    "# Save action history\n",
    "action_history_df = pd.DataFrame({\n",
    "    'session_idx': range(len(policy.action_history)),\n",
    "    'action': policy.action_history\n",
    "})\n",
    "action_history_df.to_csv('data/action_history.csv', index=False)\n",
    "print(\"Action history saved to 'data/action_history.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e555dd2-f2e7-44b0-89c8-b1cb01c8057d",
    "_uuid": "12306c9f-e5fd-445a-a126-4fac9c61b263",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Part C: Adaptive Coaching Generation\n",
    "# \n",
    "## This section implements personalized coaching cards and scenario generation\n",
    "## based on the LinUCB policy decisions from Part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "221e1aaa-47d2-4db8-a01f-0bf0a6ca5523",
    "_uuid": "bcfb4fbf-89b6-49d2-b5ed-062cc57f1659",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.817064Z",
     "iopub.status.busy": "2025-11-08T19:20:52.816893Z",
     "iopub.status.idle": "2025-11-08T19:20:52.838152Z",
     "shell.execute_reply": "2025-11-08T19:20:52.837601Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.817052Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd0d65c5-9643-429d-af20-be77694ca2e3",
    "_uuid": "16e72969-4eab-4b55-aa1e-1da06a777de2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 1. Coaching Card Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79644dc8-1029-4a96-8e2d-d5918edfced7",
    "_uuid": "d571a7a0-80e2-422b-8705-92843c7670a2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.839073Z",
     "iopub.status.busy": "2025-11-08T19:20:52.838873Z",
     "iopub.status.idle": "2025-11-08T19:20:52.880252Z",
     "shell.execute_reply": "2025-11-08T19:20:52.879499Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.839051Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CoachingCard:\n",
    "    \"\"\"Structure for a coaching card.\"\"\"\n",
    "    session_idx: int\n",
    "    focus_skill: str\n",
    "    why_this_focus: str\n",
    "    micro_exercises: List[str]\n",
    "    word_count: int\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'session_idx': self.session_idx,\n",
    "            'focus_skill': self.focus_skill,\n",
    "            'why_this_focus': self.why_this_focus,\n",
    "            'micro_exercises': self.micro_exercises,\n",
    "            'word_count': self.word_count\n",
    "        }\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        card = f\"=== COACHING CARD: Session {self.session_idx + 1} ===\\n\\n\"\n",
    "        card += f\"ðŸŽ¯ FOCUS: {self.focus_skill.replace('_', ' ').title()}\\n\\n\"\n",
    "        card += f\"WHY THIS FOCUS:\\n{self.why_this_focus}\\n\\n\"\n",
    "        card += \"MICRO-EXERCISES:\\n\"\n",
    "        for i, exercise in enumerate(self.micro_exercises, 1):\n",
    "            card += f\"{i}. {exercise}\\n\"\n",
    "        card += f\"\\n[Word count: {self.word_count}]\"\n",
    "        return card\n",
    "\n",
    "\n",
    "class CoachingCardGenerator:\n",
    "    \"\"\"Generate personalized coaching cards based on policy decisions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.skill_descriptions = {\n",
    "            'clarity': 'Clarity & Enthusiasm',\n",
    "            'active_listening': 'Active Listening & Objection Handling',\n",
    "            'call_to_action': 'Effective Call to Action',\n",
    "            'friendliness': 'Friendliness & Respectful Tone'\n",
    "        }\n",
    "        \n",
    "        # Exercise templates for each skill\n",
    "        self.exercise_templates = {\n",
    "            'clarity': [\n",
    "                \"Record yourself explaining {game} in 30 seconds. Count filler words ('um', 'like'). Aim for under 3.\",\n",
    "                \"Write your top 3 selling points for {game}. Practice delivering each in one breath.\",\n",
    "                \"Before your next call, create a 'hook sentence' that captures {game}'s unique appeal in under 15 words.\"\n",
    "            ],\n",
    "            'active_listening': [\n",
    "                \"During your next conversation, repeat back the prospect's concern in your own words before responding.\",\n",
    "                \"Create a list of 5 common objections. For each, write a validating phrase (e.g., 'I hear that...').\",\n",
    "                \"Practice the 3-second pause: After they finish speaking, count to 3 before you respond. Notice what you catch.\"\n",
    "            ],\n",
    "            'call_to_action': [\n",
    "                \"Write 3 versions of your closing invitation. Each must include a specific day/time option.\",\n",
    "                \"Practice saying: 'Based on what you've shared, I think [specific session] would be perfect. Can you join us Thursday at 7 PM?'\",\n",
    "                \"Role-play with a friend: Give them an objection, then practice pivoting to a concrete next step.\"\n",
    "            ],\n",
    "            'friendliness': [\n",
    "                \"Start your next call by finding one genuine commonality with the prospect within the first 2 minutes.\",\n",
    "                \"Record yourself speaking. Listen back: Do you sound like you're smiling? Adjust your tone to sound warmer.\",\n",
    "                \"Practice using their name 2-3 times naturally during the conversation. Notice how it changes the dynamic.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Reason templates based on transcript signals\n",
    "        self.reason_templates = {\n",
    "            'clarity': {\n",
    "                'low_enthusiasm': \"Your transcript shows lower enthusiasm scores ({enthusiasm:.2f}). Injecting more genuine excitement helps prospects feel your passion for {game}.\",\n",
    "                'low_cta': \"Your call-to-action clarity scored {cta_clarity:.2f}. Clearer invitations make it easier for prospects to say yes.\",\n",
    "                'general': \"Your clarity metrics suggest room for sharper, more compelling messaging about {game}'s value.\"\n",
    "            },\n",
    "            'active_listening': {\n",
    "                'low_objection': \"Your objection handling score is {objection_handling:.2f}. Better acknowledgment of concerns builds trust faster.\",\n",
    "                'low_questions': \"You asked questions in only {question_ratio:.0%} of turns. More questions = deeper understanding of their needs.\",\n",
    "                'general': \"Your transcript shows opportunities to demonstrate deeper listening and validation of concerns.\"\n",
    "            },\n",
    "            'call_to_action': {\n",
    "                'low_cta': \"Your CTA clarity scored {cta_clarity:.2f}. Specific, actionable invitations convert better than vague suggestions.\",\n",
    "                'low_collaborative': \"Your collaborative score is {collaborative_score:.2f}. Frame CTAs as 'next steps together' rather than pressure.\",\n",
    "                'general': \"Your closing invitations could be more concrete and easier for prospects to act on.\"\n",
    "            },\n",
    "            'friendliness': {\n",
    "                'low_empathy': \"Your empathy score is {empathy_score:.2f}. Warmer connection early makes the whole conversation flow better.\",\n",
    "                'low_social': \"Social proof score: {social_proof:.2f}. Mentioning community experiences makes invitations feel less intimidating.\",\n",
    "                'general': \"Your tone could benefit from more warmth and personal connection to put prospects at ease.\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _select_reason_template(self, focus_skill: str, features: Dict[str, float]) -> str:\n",
    "        \"\"\"Select appropriate reason template based on feature scores.\"\"\"\n",
    "        templates = self.reason_templates[focus_skill]\n",
    "        \n",
    "        if focus_skill == 'clarity':\n",
    "            if features['enthusiasm'] < 0.5:\n",
    "                return templates['low_enthusiasm']\n",
    "            elif features['cta_clarity'] < 0.5:\n",
    "                return templates['low_cta']\n",
    "            return templates['general']\n",
    "        \n",
    "        elif focus_skill == 'active_listening':\n",
    "            if features['objection_handling'] < 0.5:\n",
    "                return templates['low_objection']\n",
    "            elif features['question_ratio'] < 0.3:\n",
    "                return templates['low_questions']\n",
    "            return templates['general']\n",
    "        \n",
    "        elif focus_skill == 'call_to_action':\n",
    "            if features['cta_clarity'] < 0.5:\n",
    "                return templates['low_cta']\n",
    "            elif features['collaborative_score'] < 0.5:\n",
    "                return templates['low_collaborative']\n",
    "            return templates['general']\n",
    "        \n",
    "        else:  # friendliness\n",
    "            if features['empathy_score'] < 0.5:\n",
    "                return templates['low_empathy']\n",
    "            elif features['social_proof'] < 0.5:\n",
    "                return templates['low_social']\n",
    "            return templates['general']\n",
    "    \n",
    "    def generate_card(self, session_idx: int, focus_skill: str, \n",
    "                     features: Dict[str, float], \n",
    "                     game: str = \"the game\") -> CoachingCard:\n",
    "        \"\"\"Generate a complete coaching card.\"\"\"\n",
    "        \n",
    "        # Generate \"why this focus\" (2-3 lines)\n",
    "        reason_template = self._select_reason_template(focus_skill, features)\n",
    "        why_text = reason_template.format(**features, game=game)\n",
    "        \n",
    "        # Select 3 exercises\n",
    "        exercises = [ex.format(game=game) for ex in self.exercise_templates[focus_skill]]\n",
    "        \n",
    "        # Calculate word count\n",
    "        full_text = why_text + \" \" + \" \".join(exercises)\n",
    "        word_count = len(full_text.split())\n",
    "        \n",
    "        # Ensure within 120-180 word limit\n",
    "        if word_count > 180:\n",
    "            # Truncate exercises if needed\n",
    "            while word_count > 180 and len(exercises) > 1:\n",
    "                exercises = exercises[:-1]\n",
    "                full_text = why_text + \" \" + \" \".join(exercises)\n",
    "                word_count = len(full_text.split())\n",
    "        \n",
    "        return CoachingCard(\n",
    "            session_idx=session_idx,\n",
    "            focus_skill=self.skill_descriptions[focus_skill],\n",
    "            why_this_focus=why_text,\n",
    "            micro_exercises=exercises,\n",
    "            word_count=word_count\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c289668b-6c01-4c5e-988b-68a326361de6",
    "_uuid": "61a69b97-413c-4781-89d5-679a64baf6d4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 2. Scenario Stub Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "033a137b-e27f-4d42-9f5e-682eb656600e",
    "_uuid": "c422df26-0288-4e7a-a8ac-41b6b90719eb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.881259Z",
     "iopub.status.busy": "2025-11-08T19:20:52.881038Z",
     "iopub.status.idle": "2025-11-08T19:20:52.909685Z",
     "shell.execute_reply": "2025-11-08T19:20:52.909132Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.881242Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScenarioStub:\n",
    "    \"\"\"Structure for a practice scenario stub.\"\"\"\n",
    "    session_idx: int\n",
    "    focus_skill: str\n",
    "    difficulty: float\n",
    "    persona: str\n",
    "    opening_objection: str\n",
    "    follow_ups: List[str]\n",
    "    difficulty_toggles: Dict[str, str]\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'session_idx': self.session_idx,\n",
    "            'focus_skill': self.focus_skill,\n",
    "            'difficulty': self.difficulty,\n",
    "            'persona': self.persona,\n",
    "            'opening_objection': self.opening_objection,\n",
    "            'follow_ups': self.follow_ups,\n",
    "            'difficulty_toggles': self.difficulty_toggles\n",
    "        }\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        stub = f\"=== PRACTICE SCENARIO: Session {self.session_idx + 1} ===\\n\\n\"\n",
    "        stub += f\"ðŸŽ® FOCUS SKILL: {self.focus_skill}\\n\"\n",
    "        stub += f\"ðŸ“Š BASE DIFFICULTY: {self.difficulty}\\n\\n\"\n",
    "        stub += f\"PERSONA: {self.persona}\\n\\n\"\n",
    "        stub += f\"OPENING:\\n\\\"{self.opening_objection}\\\"\\n\\n\"\n",
    "        stub += \"FOLLOW-UP CHALLENGES:\\n\"\n",
    "        for i, followup in enumerate(self.follow_ups, 1):\n",
    "            stub += f\"{i}. \\\"{followup}\\\"\\n\"\n",
    "        stub += f\"\\nðŸ”§ DIFFICULTY TOGGLES (0.55):\\n\"\n",
    "        for toggle, description in self.difficulty_toggles.items():\n",
    "            stub += f\"  [{toggle}] {description}\\n\"\n",
    "        return stub\n",
    "\n",
    "\n",
    "class ScenarioGenerator:\n",
    "    \"\"\"Generate practice scenarios tailored to focus skills.\"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed: int = 42):\n",
    "        self.rng = random.Random(random_seed)\n",
    "        \n",
    "        # Persona templates\n",
    "        self.personas = {\n",
    "            'clarity': [\n",
    "                \"Busy parent, 35, skeptical of gaming, needs clear value proposition\",\n",
    "                \"College student, 20, distracted, needs compelling hook to stay engaged\",\n",
    "                \"Professional, 42, time-conscious, wants efficiency and clear benefits\"\n",
    "            ],\n",
    "            'active_listening': [\n",
    "                \"Former gamer, 28, burned out, has specific past negative experiences\",\n",
    "                \"Anxious newcomer, 25, worried about skill level and fitting in\",\n",
    "                \"Critical thinker, 38, asks probing questions to test your knowledge\"\n",
    "            ],\n",
    "            'call_to_action': [\n",
    "                \"Interested but non-committal, 30, needs gentle push to concrete action\",\n",
    "                \"Over-thinker, 33, gets stuck in analysis paralysis\",\n",
    "                \"Schedule-juggler, 29, needs very specific timing options\"\n",
    "            ],\n",
    "            'friendliness': [\n",
    "                \"Shy introvert, 24, needs warmth to open up about interests\",\n",
    "                \"Defensive skeptic, 36, testing whether you're 'just selling'\",\n",
    "                \"Lonely player, 31, craves community but fears rejection\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Opening objections tailored to each skill\n",
    "        self.opening_objections = {\n",
    "            'clarity': [\n",
    "                \"I've heard of this game, but I'm not really sure what makes it special. Can you explain?\",\n",
    "                \"Okay, so... what exactly would I be doing? I'm kind of confused about the gameplay.\",\n",
    "                \"Why should I play this instead of [popular alternative]? What's the difference?\"\n",
    "            ],\n",
    "            'active_listening': [\n",
    "                \"I tried a game like this before and the community was toxic. How is this different?\",\n",
    "                \"I'm really bad at these types of games. I don't want to embarrass myself.\",\n",
    "                \"I'm interested, but I have some concerns about the time commitment and learning curve.\"\n",
    "            ],\n",
    "            'call_to_action': [\n",
    "                \"This sounds interesting, but I need to think about it. Can I get back to you?\",\n",
    "                \"I might be interested... maybe sometime next month when things calm down?\",\n",
    "                \"I'm not sure I'm ready to commit to a session yet. What if I just lurk first?\"\n",
    "            ],\n",
    "            'friendliness': [\n",
    "                \"I don't know anyone who plays this. Would I just be joining random strangers?\",\n",
    "                \"Are you getting paid to recruit people? This feels like a sales pitch.\",\n",
    "                \"I'm not really a 'joiner' type. I prefer playing solo.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Follow-ups that stress the specific skill\n",
    "        self.follow_ups = {\n",
    "            'clarity': [\n",
    "                \"You're giving me a lot of information... can you just give me the main reason to try it?\",\n",
    "                \"I still don't get it. What would my first hour actually look like?\"\n",
    "            ],\n",
    "            'active_listening': [\n",
    "                \"It sounds like you're just reading from a script. Did you hear what I just said about my schedule?\",\n",
    "                \"You keep talking about features, but I told you my main concern is [X]. Can you address that?\"\n",
    "            ],\n",
    "            'call_to_action': [\n",
    "                \"That's a lot to consider. What exactly are you asking me to do right now?\",\n",
    "                \"I hear you, but 'sometime soon' is too vague. When specifically works for me?\"\n",
    "            ],\n",
    "            'friendliness': [\n",
    "                \"You seem nice, but I'm still not convinced this is for someone like me.\",\n",
    "                \"How do I know the other players will be welcoming? I've had bad experiences.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Difficulty toggles for 0.55 difficulty\n",
    "        self.difficulty_toggles = {\n",
    "            'clarity': {\n",
    "                'Time Pressure': 'They interrupt after 90 seconds: \"Can you wrap this up? I have another call.\"',\n",
    "                'Stronger Skepticism': 'They push back twice as hard: \"My friend said this game is overhyped. Prove them wrong.\"'\n",
    "            },\n",
    "            'active_listening': {\n",
    "                'Multiple Concerns': 'They layer 3 objections at once instead of 1.',\n",
    "                'Emotional Intensity': 'They sound frustrated/anxious; mishandling tone ends the call.'\n",
    "            },\n",
    "            'call_to_action': {\n",
    "                'Time Pressure': 'You have only 2 minutes to secure commitment before they \"have to go.\"',\n",
    "                'Analysis Paralysis': 'They ask for 3+ more details before committing; you must redirect firmly but kindly.'\n",
    "            },\n",
    "            'friendliness': {\n",
    "                'Cold Opening': 'They start defensive: \"Make this quick. I don\\'t have much time.\"',\n",
    "                'Trust Test': 'They explicitly test if you\\'re authentic: \"Are you reading from a script?\"'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_scenario(self, session_idx: int, focus_skill: str, \n",
    "                         difficulty: float = 0.45) -> ScenarioStub:\n",
    "        \"\"\"Generate a practice scenario stub.\"\"\"\n",
    "        \n",
    "        # Select random persona and objections\n",
    "        persona = self.rng.choice(self.personas[focus_skill])\n",
    "        opening = self.rng.choice(self.opening_objections[focus_skill])\n",
    "        followups = self.follow_ups[focus_skill].copy()\n",
    "        \n",
    "        return ScenarioStub(\n",
    "            session_idx=session_idx,\n",
    "            focus_skill=focus_skill.replace('_', ' ').title(),\n",
    "            difficulty=difficulty,\n",
    "            persona=persona,\n",
    "            opening_objection=opening,\n",
    "            follow_ups=followups,\n",
    "            difficulty_toggles=self.difficulty_toggles[focus_skill]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3593d05-453e-40f6-be54-18efed72d718",
    "_uuid": "76ce1d2a-b0d7-423d-9817-e61783b148ca",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 3. Integration with Policy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dec91ca4-d720-4c5a-a5d1-0fd8192d15fa",
    "_uuid": "52231322-4d87-4771-bb04-41034ce9bb9a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.910605Z",
     "iopub.status.busy": "2025-11-08T19:20:52.910357Z",
     "iopub.status.idle": "2025-11-08T19:20:52.939118Z",
     "shell.execute_reply": "2025-11-08T19:20:52.938566Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.910590Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_coaching_pipeline(policy_df: pd.DataFrame, \n",
    "                               features_df: pd.DataFrame,\n",
    "                               output_dir: str = 'data') -> Tuple[List[CoachingCard], List[ScenarioStub]]:\n",
    "    \"\"\"\n",
    "    Generate coaching cards and scenarios for all sessions.\n",
    "    \n",
    "    Args:\n",
    "        policy_df: DataFrame with policy results (from Part B)\n",
    "        features_df: DataFrame with LLM features (from Part A)\n",
    "        output_dir: Directory to save outputs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (coaching_cards, scenario_stubs)\n",
    "    \"\"\"\n",
    "    card_gen = CoachingCardGenerator()\n",
    "    scenario_gen = ScenarioGenerator()\n",
    "    \n",
    "    coaching_cards = []\n",
    "    scenario_stubs = []\n",
    "    \n",
    "    # Merge dataframes\n",
    "    merged = policy_df.merge(\n",
    "        features_df[['conversation_id', 'objection_handling', 'question_ratio', \n",
    "                     'cta_clarity', 'empathy_score', 'collaborative_score', \n",
    "                     'social_proof', 'enthusiasm']], \n",
    "        on='conversation_id'\n",
    "    )\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        # Get focus skill (map back to action name)\n",
    "        focus_skill = row['selected_action']\n",
    "        \n",
    "        # Extract features\n",
    "        features = {\n",
    "            'objection_handling': row['objection_handling'],\n",
    "            'question_ratio': row['question_ratio'],\n",
    "            'cta_clarity': row['cta_clarity'],\n",
    "            'empathy_score': row['empathy_score'],\n",
    "            'collaborative_score': row['collaborative_score'],\n",
    "            'social_proof': row['social_proof'],\n",
    "            'enthusiasm': row['enthusiasm']\n",
    "        }\n",
    "        \n",
    "        # Generate coaching card\n",
    "        card = card_gen.generate_card(\n",
    "            session_idx=idx,\n",
    "            focus_skill=focus_skill,\n",
    "            features=features,\n",
    "            game=\"our gaming community\"\n",
    "        )\n",
    "        coaching_cards.append(card)\n",
    "        \n",
    "        # Generate scenario stub\n",
    "        scenario = scenario_gen.generate_scenario(\n",
    "            session_idx=idx,\n",
    "            focus_skill=focus_skill,\n",
    "            difficulty=0.45\n",
    "        )\n",
    "        scenario_stubs.append(scenario)\n",
    "    \n",
    "    return coaching_cards, scenario_stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3534a8ff-a537-4cde-a263-1c537448c996",
    "_uuid": "9525626c-ce9c-489f-bc9a-24cb8ddffd4e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 4. Run Coaching Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d0495fca-0c27-438b-9441-8e6e2e1ee6d2",
    "_uuid": "6f8beee7-5af0-45d5-8a62-c0e5196120a0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.940139Z",
     "iopub.status.busy": "2025-11-08T19:20:52.939858Z",
     "iopub.status.idle": "2025-11-08T19:20:52.968818Z",
     "shell.execute_reply": "2025-11-08T19:20:52.968094Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.940124Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate coaching materials\n",
    "coaching_cards, scenario_stubs = generate_coaching_pipeline(\n",
    "    policy_df=policy_df,\n",
    "    features_df=features_df,\n",
    "    output_dir='data'\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(coaching_cards)} coaching cards and {len(scenario_stubs)} scenario stubs\")\n",
    "\n",
    "# Display first 3 examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE COACHING OUTPUTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(min(3, len(coaching_cards))):\n",
    "    print(f\"\\n{coaching_cards[i]}\")\n",
    "    print(f\"\\n{scenario_stubs[i]}\")\n",
    "    print(\"\\n\" + \"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7d24f0f-1d36-411b-9dbb-184faaefe7d7",
    "_uuid": "2917ae82-bc44-45bb-9915-1475c041bc7a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 5. Save Coaching Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e31171d-820b-4a9c-9fad-76ae5a329c6d",
    "_uuid": "3e808511-0dc5-46bb-b220-be98feb77afb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.969594Z",
     "iopub.status.busy": "2025-11-08T19:20:52.969424Z",
     "iopub.status.idle": "2025-11-08T19:20:52.988090Z",
     "shell.execute_reply": "2025-11-08T19:20:52.987467Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.969581Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/coaching_materials', exist_ok=True)\n",
    "\n",
    "# Save coaching cards\n",
    "cards_data = [card.to_dict() for card in coaching_cards]\n",
    "with open('data/coaching_materials/coaching_cards.json', 'w') as f:\n",
    "    json.dump(cards_data, f, indent=2)\n",
    "\n",
    "# Save as readable text\n",
    "with open('data/coaching_materials/coaching_cards.txt', 'w') as f:\n",
    "    for card in coaching_cards:\n",
    "        f.write(str(card))\n",
    "        f.write(\"\\n\\n\" + \"=\"*70 + \"\\n\\n\")\n",
    "\n",
    "# Save scenario stubs\n",
    "scenarios_data = [scenario.to_dict() for scenario in scenario_stubs]\n",
    "with open('data/coaching_materials/scenario_stubs.json', 'w') as f:\n",
    "    json.dump(scenarios_data, f, indent=2)\n",
    "\n",
    "# Save as readable text\n",
    "with open('data/coaching_materials/scenario_stubs.txt', 'w') as f:\n",
    "    for scenario in scenario_stubs:\n",
    "        f.write(str(scenario))\n",
    "        f.write(\"\\n\\n\" + \"=\"*70 + \"\\n\\n\")\n",
    "\n",
    "print(\"âœ… Coaching materials saved to 'data/coaching_materials/'\")\n",
    "print(\"   - coaching_cards.json & .txt\")\n",
    "print(\"   - scenario_stubs.json & .txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a6bf706-aa10-4dcb-bf14-ff6d184d8bb4",
    "_uuid": "58d2754a-944f-4007-90e1-d885f4c2ed53",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 6. Coaching Effectiveness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65d7f4af-7956-46d2-962d-21886cf4ddc1",
    "_uuid": "02ffe77d-e6c9-444f-b87e-6e29385eca8f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:52.989258Z",
     "iopub.status.busy": "2025-11-08T19:20:52.988955Z",
     "iopub.status.idle": "2025-11-08T19:20:53.156375Z",
     "shell.execute_reply": "2025-11-08T19:20:53.155817Z",
     "shell.execute_reply.started": "2025-11-08T19:20:52.989243Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze coaching card statistics\n",
    "card_stats = pd.DataFrame([\n",
    "    {\n",
    "        'session_idx': card.session_idx,\n",
    "        'focus_skill': card.focus_skill,\n",
    "        'word_count': card.word_count,\n",
    "        'num_exercises': len(card.micro_exercises)\n",
    "    }\n",
    "    for card in coaching_cards\n",
    "])\n",
    "\n",
    "print(\"\\nCoaching Card Statistics:\")\n",
    "print(card_stats.describe())\n",
    "\n",
    "# Focus skill distribution\n",
    "skill_dist = card_stats['focus_skill'].value_counts()\n",
    "print(\"\\nFocus Skill Distribution:\")\n",
    "print(skill_dist)\n",
    "\n",
    "# Visualize\n",
    "import plotly.express as px\n",
    "fig = px.bar(skill_dist, \n",
    "             title='Distribution of Focus Skills in Coaching Cards',\n",
    "             labels={'value': 'Count', 'index': 'Focus Skill'})\n",
    "fig.show()\n",
    "\n",
    "# Word count distribution\n",
    "fig = px.histogram(card_stats, x='word_count',\n",
    "                   title='Coaching Card Word Count Distribution',\n",
    "                   labels={'word_count': 'Word Count'},\n",
    "                   nbins=20)\n",
    "fig.add_vline(x=120, line_dash=\"dash\", line_color=\"green\", \n",
    "              annotation_text=\"Min (120)\")\n",
    "fig.add_vline(x=180, line_dash=\"dash\", line_color=\"red\",\n",
    "              annotation_text=\"Max (180)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14223d5e-1f24-4b02-b56f-ad32f1eae247",
    "_uuid": "f3e0cc94-1e36-4b3a-ad2b-34b41679b50b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 7. Export Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b25d434b-0a05-4234-8e64-a7de125a25db",
    "_uuid": "812333cd-676a-4fae-baab-3df3cf75ff5d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:20:53.157142Z",
     "iopub.status.busy": "2025-11-08T19:20:53.156990Z",
     "iopub.status.idle": "2025-11-08T19:20:53.171551Z",
     "shell.execute_reply": "2025-11-08T19:20:53.170815Z",
     "shell.execute_reply.started": "2025-11-08T19:20:53.157130Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a complete training session export\n",
    "def create_training_session_export(session_idx: int) -> Dict:\n",
    "    \"\"\"Create a complete training session package.\"\"\"\n",
    "    return {\n",
    "        'session_number': session_idx + 1,\n",
    "        'policy_decision': {\n",
    "            'selected_action': policy_df.iloc[session_idx]['selected_action'],\n",
    "            'reward': policy_df.iloc[session_idx]['reward'],\n",
    "            'ucb_scores': {\n",
    "                action: policy_df.iloc[session_idx][f'ucb_{action}']\n",
    "                for action in ['clarity', 'active_listening', 'call_to_action', 'friendliness']\n",
    "            }\n",
    "        },\n",
    "        'coaching_card': coaching_cards[session_idx].to_dict(),\n",
    "        'practice_scenario': scenario_stubs[session_idx].to_dict(),\n",
    "        'current_performance': {\n",
    "            'overall_score': policy_df.iloc[session_idx]['overall_score'],\n",
    "            'improvement_rate': policy_df.iloc[session_idx]['improvement_rate']\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Export all training sessions\n",
    "training_pipeline = [\n",
    "    create_training_session_export(i) \n",
    "    for i in range(len(coaching_cards))\n",
    "]\n",
    "\n",
    "with open('data/coaching_materials/complete_training_pipeline.json', 'w') as f:\n",
    "    json.dump(training_pipeline, f, indent=2)\n",
    "\n",
    "print(\"âœ… Complete training pipeline exported to 'data/coaching_materials/complete_training_pipeline.json'\")\n",
    "print(f\"\\nðŸ“Š Pipeline Summary:\")\n",
    "print(f\"   Total Sessions: {len(training_pipeline)}\")\n",
    "print(f\"   Average Word Count: {card_stats['word_count'].mean():.1f}\")\n",
    "print(f\"   Word Count Range: {card_stats['word_count'].min()}-{card_stats['word_count'].max()}\")\n",
    "print(f\"   Skills Covered: {card_stats['focus_skill'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a46b738-cf94-441b-94f2-fa3e7dc1aa8f",
    "_uuid": "6e6ba221-ed56-4986-afd1-5974cdc1e865",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Part D: Lightweight Evaluation\n",
    "# \n",
    "### This section evaluates:\n",
    "### 1. Feature usefulness: Do LLM features improve prediction of score improvements?\n",
    "### 2. Policy performance: How does LinUCB compare to baseline policies?\n",
    "### 3. Alignment sanity: Does the policy focus on actual weaknesses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac5ab084-40a6-46bb-9257-5397dac43416",
    "_uuid": "e75c9dad-77f9-4894-bbaa-f3b623419752",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:41:41.764554Z",
     "iopub.status.busy": "2025-11-08T19:41:41.764046Z",
     "iopub.status.idle": "2025-11-08T19:41:41.768950Z",
     "shell.execute_reply": "2025-11-08T19:41:41.768279Z",
     "shell.execute_reply.started": "2025-11-08T19:41:41.764531Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import r2_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from typing import Dict, List, Tuple\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7fb163ac-a0b2-4dd8-afac-87108da40c47",
    "_uuid": "4714f50d-0715-4720-8fd1-ec55563f7452",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 1. Feature Usefulness Evaluation\n",
    "# \n",
    "### We'll use Leave-One-Step-Out (LOSO) cross-validation to predict:\n",
    "### - Regression: Î”overall (continuous improvement score)\n",
    "### - Classification: Positive Î”overall (yes/no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c09b55f3-d177-4271-9e68-6d27662a115c",
    "_uuid": "5a6a2278-57ff-4c61-8118-619adca24dbe",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:41:45.007049Z",
     "iopub.status.busy": "2025-11-08T19:41:45.006338Z",
     "iopub.status.idle": "2025-11-08T19:41:45.021025Z",
     "shell.execute_reply": "2025-11-08T19:41:45.020332Z",
     "shell.execute_reply.started": "2025-11-08T19:41:45.007024Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureUsefulnessEvaluator:\n",
    "    \"\"\"Evaluate predictive power of LLM features using LOSO.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize evaluator.\n",
    "        \n",
    "        Args:\n",
    "            data_df: Merged dataframe with features, rubric scores, and metadata\n",
    "        \"\"\"\n",
    "        self.data = data_df.copy()\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare features and targets for evaluation.\"\"\"\n",
    "        # Calculate Î”overall (tâ†’t+1)\n",
    "        self.data['delta_overall'] = self.data['Overall'].diff().shift(-1)\n",
    "        self.data['positive_delta'] = (self.data['delta_overall'] > 0).astype(int)\n",
    "        \n",
    "        # Remove last row (no next step)\n",
    "        self.data = self.data[:-1].copy()\n",
    "        \n",
    "        # Define feature sets\n",
    "        self.rubric_features = [\n",
    "            'clarity and enthusiasm in pitch',\n",
    "            'active listening and objection handling',\n",
    "            'effective call to action',\n",
    "            'friendliness and respectful tone'\n",
    "        ]\n",
    "        \n",
    "        self.llm_features = [\n",
    "            'objection_handling', 'question_ratio', 'cta_clarity',\n",
    "            'empathy_score', 'collaborative_score', 'social_proof', \n",
    "            'enthusiasm'\n",
    "        ]\n",
    "        \n",
    "        # Add metadata features\n",
    "        self.metadata_features = ['duration']  # talk ratio could be added if available\n",
    "        \n",
    "        # Baseline: rubric + metadata\n",
    "        self.baseline_features = self.rubric_features + self.metadata_features\n",
    "        \n",
    "        # Full: baseline + LLM features\n",
    "        self.full_features = self.baseline_features + self.llm_features\n",
    "    \n",
    "    def _loso_regression(self, features: List[str]) -> Dict:\n",
    "        \"\"\"Perform LOSO regression to predict Î”overall.\"\"\"\n",
    "        X = self.data[features].values\n",
    "        y = self.data['delta_overall'].values\n",
    "        \n",
    "        loo = LeaveOneOut()\n",
    "        predictions = np.zeros(len(y))\n",
    "        \n",
    "        for train_idx, test_idx in loo.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train = y[train_idx]\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions[test_idx] = model.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y, predictions)\n",
    "        mae = np.mean(np.abs(y - predictions))\n",
    "        \n",
    "        return {\n",
    "            'r2': r2,\n",
    "            'mae': mae,\n",
    "            'predictions': predictions,\n",
    "            'actuals': y\n",
    "        }\n",
    "    \n",
    "    def _loso_classification(self, features: List[str]) -> Dict:\n",
    "        \"\"\"Perform LOSO classification to predict positive Î”overall.\"\"\"\n",
    "        X = self.data[features].values\n",
    "        y = self.data['positive_delta'].values\n",
    "        \n",
    "        loo = LeaveOneOut()\n",
    "        predictions = np.zeros(len(y))\n",
    "        probabilities = np.zeros(len(y))\n",
    "        \n",
    "        for train_idx, test_idx in loo.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train = y[train_idx]\n",
    "            \n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions[test_idx] = model.predict(X_test)\n",
    "            probabilities[test_idx] = model.predict_proba(X_test)[0, 1]\n",
    "        \n",
    "        accuracy = accuracy_score(y, predictions)\n",
    "        auc = roc_auc_score(y, probabilities)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'predictions': predictions,\n",
    "            'probabilities': probabilities,\n",
    "            'actuals': y\n",
    "        }\n",
    "    \n",
    "    def run_ablation_study(self) -> pd.DataFrame:\n",
    "        \"\"\"Run complete ablation study comparing feature sets.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # 1. Baseline (rubric + metadata)\n",
    "        print(\"Evaluating baseline features...\")\n",
    "        reg_baseline = self._loso_regression(self.baseline_features)\n",
    "        clf_baseline = self._loso_classification(self.baseline_features)\n",
    "        \n",
    "        results.append({\n",
    "            'Feature Set': 'Baseline (Rubric + Metadata)',\n",
    "            'RÂ² (Regression)': reg_baseline['r2'],\n",
    "            'MAE (Regression)': reg_baseline['mae'],\n",
    "            'Accuracy (Classification)': clf_baseline['accuracy'],\n",
    "            'AUC (Classification)': clf_baseline['auc'],\n",
    "            'Num Features': len(self.baseline_features)\n",
    "        })\n",
    "        \n",
    "        # 2. Full (baseline + LLM)\n",
    "        print(\"Evaluating full feature set...\")\n",
    "        reg_full = self._loso_regression(self.full_features)\n",
    "        clf_full = self._loso_classification(self.full_features)\n",
    "        \n",
    "        results.append({\n",
    "            'Feature Set': 'Full (Baseline + LLM)',\n",
    "            'RÂ² (Regression)': reg_full['r2'],\n",
    "            'MAE (Regression)': reg_full['mae'],\n",
    "            'Accuracy (Classification)': clf_full['accuracy'],\n",
    "            'AUC (Classification)': clf_full['auc'],\n",
    "            'Num Features': len(self.full_features)\n",
    "        })\n",
    "        \n",
    "        # 3. LLM only\n",
    "        print(\"Evaluating LLM features only...\")\n",
    "        reg_llm = self._loso_regression(self.llm_features)\n",
    "        clf_llm = self._loso_classification(self.llm_features)\n",
    "        \n",
    "        results.append({\n",
    "            'Feature Set': 'LLM Only',\n",
    "            'RÂ² (Regression)': reg_llm['r2'],\n",
    "            'MAE (Regression)': reg_llm['mae'],\n",
    "            'Accuracy (Classification)': clf_llm['accuracy'],\n",
    "            'AUC (Classification)': clf_llm['auc'],\n",
    "            'Num Features': len(self.llm_features)\n",
    "        })\n",
    "        \n",
    "        # Calculate improvements\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Store detailed results for plotting\n",
    "        self.regression_results = {\n",
    "            'baseline': reg_baseline,\n",
    "            'full': reg_full,\n",
    "            'llm_only': reg_llm\n",
    "        }\n",
    "        self.classification_results = {\n",
    "            'baseline': clf_baseline,\n",
    "            'full': clf_full,\n",
    "            'llm_only': clf_llm\n",
    "        }\n",
    "        \n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0cbac31-39aa-482f-90ea-cdbc10086813",
    "_uuid": "b6b02349-284c-4a0b-acaa-6ff655d0dd10",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 2. Run Feature Usefulness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee2907a4-38cd-4640-a2c0-488f9b3e735f",
    "_uuid": "511a2123-3732-4be7-b67e-ba43f00345dc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:41:49.395004Z",
     "iopub.status.busy": "2025-11-08T19:41:49.394702Z",
     "iopub.status.idle": "2025-11-08T19:41:50.118186Z",
     "shell.execute_reply": "2025-11-08T19:41:50.117526Z",
     "shell.execute_reply.started": "2025-11-08T19:41:49.394984Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize evaluator with merged data\n",
    "evaluator = FeatureUsefulnessEvaluator(full_df)\n",
    "\n",
    "# Run ablation study\n",
    "ablation_results = evaluator.run_ablation_study()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(ablation_results.to_string(index=False))\n",
    "\n",
    "# Calculate improvement from baseline to full\n",
    "baseline_r2 = ablation_results.iloc[0]['RÂ² (Regression)']\n",
    "full_r2 = ablation_results.iloc[1]['RÂ² (Regression)']\n",
    "r2_improvement = ((full_r2 - baseline_r2) / abs(baseline_r2)) * 100 if baseline_r2 != 0 else float('inf')\n",
    "\n",
    "baseline_auc = ablation_results.iloc[0]['AUC (Classification)']\n",
    "full_auc = ablation_results.iloc[1]['AUC (Classification)']\n",
    "auc_improvement = ((full_auc - baseline_auc) / baseline_auc) * 100\n",
    "\n",
    "print(f\"\\nðŸ“Š LLM Feature Impact:\")\n",
    "print(f\"   RÂ² Improvement: {r2_improvement:+.1f}%\")\n",
    "print(f\"   AUC Improvement: {auc_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "48be3465-a983-4150-8686-1e61b68b00f0",
    "_uuid": "c7b53ef9-fa3f-42dc-92ce-8af6e3511e0a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 3. Visualize Prediction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1de86707-b9b3-44d4-be35-230fa0bc5a54",
    "_uuid": "b067b5fa-a56c-4d7f-9c58-6d3b76baf55e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:41:55.074746Z",
     "iopub.status.busy": "2025-11-08T19:41:55.074183Z",
     "iopub.status.idle": "2025-11-08T19:41:55.097870Z",
     "shell.execute_reply": "2025-11-08T19:41:55.097113Z",
     "shell.execute_reply.started": "2025-11-08T19:41:55.074723Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Regression: Actual vs Predicted\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, results in evaluator.regression_results.items():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=results['actuals'],\n",
    "        y=results['predictions'],\n",
    "        mode='markers',\n",
    "        name=name.replace('_', ' ').title(),\n",
    "        opacity=0.6\n",
    "    ))\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(evaluator.regression_results['baseline']['actuals'].min(),\n",
    "              evaluator.regression_results['full']['actuals'].min())\n",
    "max_val = max(evaluator.regression_results['baseline']['actuals'].max(),\n",
    "              evaluator.regression_results['full']['actuals'].max())\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[min_val, max_val],\n",
    "    y=[min_val, max_val],\n",
    "    mode='lines',\n",
    "    name='Perfect Prediction',\n",
    "    line=dict(dash='dash', color='gray')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Regression: Actual vs Predicted Î”overall (LOSO)',\n",
    "    xaxis_title='Actual Î”overall',\n",
    "    yaxis_title='Predicted Î”overall',\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Classification: ROC-style comparison\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for name, results in evaluator.classification_results.items():\n",
    "    fpr, tpr, _ = roc_curve(results['actuals'], results['probabilities'])\n",
    "    auc = results['auc']\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode='lines',\n",
    "        name=f\"{name.replace('_', ' ').title()} (AUC={auc:.3f})\"\n",
    "    ))\n",
    "\n",
    "# Add random classifier line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1],\n",
    "    y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Random Classifier',\n",
    "    line=dict(dash='dash', color='gray')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ROC Curves: Predicting Positive Î”overall (LOSO)',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7127a661-f8e7-4186-9d04-6a551e6af4e4",
    "_uuid": "91942aae-e85f-4502-b4eb-d483b27d3adb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 4. Policy Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "53f4afbc-474d-4861-bc57-f71be0ab7b4f",
    "_uuid": "341ea152-3180-471a-b66c-a8fe739a9864",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:41:59.874327Z",
     "iopub.status.busy": "2025-11-08T19:41:59.874051Z",
     "iopub.status.idle": "2025-11-08T19:41:59.890335Z",
     "shell.execute_reply": "2025-11-08T19:41:59.889811Z",
     "shell.execute_reply.started": "2025-11-08T19:41:59.874307Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PolicyEvaluator:\n",
    "    \"\"\"Compare LinUCB policy against baseline policies.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_df: pd.DataFrame, policy_results_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize policy evaluator.\n",
    "        \n",
    "        Args:\n",
    "            data_df: Merged dataframe with features and rubric scores\n",
    "            policy_results_df: Results from LinUCB policy\n",
    "        \"\"\"\n",
    "        self.data = data_df.copy()\n",
    "        self.linucb_results = policy_results_df.copy()\n",
    "        \n",
    "        # Map rubric columns to actions\n",
    "        self.rubric_to_action = {\n",
    "            'clarity and enthusiasm in pitch': 'clarity',\n",
    "            'active listening and objection handling': 'active_listening',\n",
    "            'effective call to action': 'call_to_action',\n",
    "            'friendliness and respectful tone': 'friendliness'\n",
    "        }\n",
    "    \n",
    "    def _weakest_skill_first_policy(self) -> pd.DataFrame:\n",
    "        \"\"\"Simulate weakest-skill-first baseline policy.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx in range(len(self.data) - 1):  # Exclude last row (no next step)\n",
    "            current_row = self.data.iloc[idx]\n",
    "            next_row = self.data.iloc[idx + 1]\n",
    "            \n",
    "            # Find weakest rubric score\n",
    "            rubric_scores = {\n",
    "                skill: current_row[rubric]\n",
    "                for rubric, skill in self.rubric_to_action.items()\n",
    "            }\n",
    "            weakest_skill = min(rubric_scores, key=rubric_scores.get)\n",
    "            \n",
    "            # Calculate reward (same as LinUCB)\n",
    "            current_rubric = {\n",
    "                rubric: current_row[rubric]\n",
    "                for rubric in self.rubric_to_action.keys()\n",
    "            }\n",
    "            current_rubric['Overall'] = current_row['Overall']\n",
    "            \n",
    "            next_rubric = {\n",
    "                rubric: next_row[rubric]\n",
    "                for rubric in self.rubric_to_action.keys()\n",
    "            }\n",
    "            next_rubric['Overall'] = next_row['Overall']\n",
    "            \n",
    "            # Map action to rubric\n",
    "            action_to_rubric = {v: k for k, v in self.rubric_to_action.items()}\n",
    "            skill_rubric = action_to_rubric[weakest_skill]\n",
    "            \n",
    "            delta_skill = (next_rubric[skill_rubric] - current_rubric[skill_rubric]) / 100.0\n",
    "            delta_overall = (next_rubric['Overall'] - current_rubric['Overall']) / 100.0\n",
    "            reward = 0.6 * delta_skill + 0.4 * delta_overall\n",
    "            \n",
    "            # Calculate Î”overall\n",
    "            delta_overall_raw = next_row['Overall'] - current_row['Overall']\n",
    "            \n",
    "            results.append({\n",
    "                'session_idx': idx,\n",
    "                'selected_action': weakest_skill,\n",
    "                'reward': reward,\n",
    "                'delta_overall': delta_overall_raw,\n",
    "                'positive_delta': delta_overall_raw > 0\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _random_policy(self, seed: int = 42) -> pd.DataFrame:\n",
    "        \"\"\"Simulate random action selection policy.\"\"\"\n",
    "        rng = np.random.RandomState(seed)\n",
    "        actions = list(self.rubric_to_action.values())\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for idx in range(len(self.data) - 1):\n",
    "            current_row = self.data.iloc[idx]\n",
    "            next_row = self.data.iloc[idx + 1]\n",
    "            \n",
    "            # Random action\n",
    "            selected_action = rng.choice(actions)\n",
    "            \n",
    "            # Calculate reward\n",
    "            current_rubric = {\n",
    "                rubric: current_row[rubric]\n",
    "                for rubric in self.rubric_to_action.keys()\n",
    "            }\n",
    "            current_rubric['Overall'] = current_row['Overall']\n",
    "            \n",
    "            next_rubric = {\n",
    "                rubric: next_row[rubric]\n",
    "                for rubric in self.rubric_to_action.keys()\n",
    "            }\n",
    "            next_rubric['Overall'] = next_row['Overall']\n",
    "            \n",
    "            action_to_rubric = {v: k for k, v in self.rubric_to_action.items()}\n",
    "            skill_rubric = action_to_rubric[selected_action]\n",
    "            \n",
    "            delta_skill = (next_rubric[skill_rubric] - current_rubric[skill_rubric]) / 100.0\n",
    "            delta_overall = (next_rubric['Overall'] - current_rubric['Overall']) / 100.0\n",
    "            reward = 0.6 * delta_skill + 0.4 * delta_overall\n",
    "            \n",
    "            delta_overall_raw = next_row['Overall'] - current_row['Overall']\n",
    "            \n",
    "            results.append({\n",
    "                'session_idx': idx,\n",
    "                'selected_action': selected_action,\n",
    "                'reward': reward,\n",
    "                'delta_overall': delta_overall_raw,\n",
    "                'positive_delta': delta_overall_raw > 0\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def compare_policies(self) -> pd.DataFrame:\n",
    "        \"\"\"Compare all policies.\"\"\"\n",
    "        # Generate baseline policies\n",
    "        weakest_results = self._weakest_skill_first_policy()\n",
    "        random_results = self._random_policy()\n",
    "        \n",
    "        # Add delta_overall to LinUCB results if not present\n",
    "        linucb_with_delta = self.linucb_results.copy()\n",
    "        if 'delta_overall' not in linucb_with_delta.columns:\n",
    "            # Calculate delta_overall from the data\n",
    "            delta_overall_list = []\n",
    "            positive_delta_list = []\n",
    "            \n",
    "            for idx in range(len(linucb_with_delta)):\n",
    "                session_idx = linucb_with_delta.iloc[idx]['session_idx']\n",
    "                if session_idx < len(self.data) - 1:\n",
    "                    current_overall = self.data.iloc[session_idx]['Overall']\n",
    "                    next_overall = self.data.iloc[session_idx + 1]['Overall']\n",
    "                    delta = next_overall - current_overall\n",
    "                    delta_overall_list.append(delta)\n",
    "                    positive_delta_list.append(delta > 0)\n",
    "                else:\n",
    "                    delta_overall_list.append(np.nan)\n",
    "                    positive_delta_list.append(False)\n",
    "            \n",
    "            linucb_with_delta['delta_overall'] = delta_overall_list\n",
    "            linucb_with_delta['positive_delta'] = positive_delta_list\n",
    "        \n",
    "        # Filter out the last session (no next step)\n",
    "        linucb_with_delta = linucb_with_delta[linucb_with_delta['session_idx'] < len(self.data) - 1]\n",
    "        \n",
    "        # Calculate metrics for each policy\n",
    "        policies = {\n",
    "            'LinUCB': linucb_with_delta,\n",
    "            'Weakest-Skill-First': weakest_results,\n",
    "            'Random': random_results\n",
    "        }\n",
    "        \n",
    "        comparison = []\n",
    "        \n",
    "        for policy_name, results_df in policies.items():\n",
    "            metrics = {\n",
    "                'Policy': policy_name,\n",
    "                'Avg Reward': results_df['reward'].mean(),\n",
    "                'Std Reward': results_df['reward'].std(),\n",
    "                'Avg Î”overall': results_df['delta_overall'].mean(),\n",
    "                '% Positive Î”overall': (results_df['positive_delta'].mean() * 100),\n",
    "                'Total Positive Steps': results_df['positive_delta'].sum(),\n",
    "                'Total Negative Steps': (~results_df['positive_delta']).sum()\n",
    "            }\n",
    "            comparison.append(metrics)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison)\n",
    "        \n",
    "        # Store for later analysis\n",
    "        self.policy_comparison = comparison_df\n",
    "        self.baseline_results = {\n",
    "            'linucb': linucb_with_delta,\n",
    "            'weakest': weakest_results,\n",
    "            'random': random_results\n",
    "        }\n",
    "        \n",
    "        return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76f5b113-b6a6-4cfa-8a4b-9945dc9d5583",
    "_uuid": "028d015a-0145-49e1-84f1-62701813d232",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 5. Run Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11ef5ad1-ff4f-48c8-9919-4d902c4a4016",
    "_uuid": "56b22333-4e4d-49d6-8947-bd462977a87c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:42:04.660243Z",
     "iopub.status.busy": "2025-11-08T19:42:04.659972Z",
     "iopub.status.idle": "2025-11-08T19:42:04.689717Z",
     "shell.execute_reply": "2025-11-08T19:42:04.688993Z",
     "shell.execute_reply.started": "2025-11-08T19:42:04.660224Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize policy evaluator\n",
    "policy_eval = PolicyEvaluator(full_df, policy_df)\n",
    "\n",
    "# Compare policies\n",
    "policy_comparison = policy_eval.compare_policies()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POLICY PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(policy_comparison.to_string(index=False))\n",
    "\n",
    "# Calculate improvements over baseline\n",
    "linucb_reward = policy_comparison.iloc[0]['Avg Reward']\n",
    "baseline_reward = policy_comparison.iloc[1]['Avg Reward']\n",
    "random_reward = policy_comparison.iloc[2]['Avg Reward']\n",
    "\n",
    "reward_vs_baseline = ((linucb_reward - baseline_reward) / abs(baseline_reward)) * 100 if baseline_reward != 0 else float('inf')\n",
    "reward_vs_random = ((linucb_reward - random_reward) / abs(random_reward)) * 100 if random_reward != 0 else float('inf')\n",
    "\n",
    "print(f\"\\nðŸ“ˆ LinUCB Performance:\")\n",
    "print(f\"   vs Weakest-Skill-First: {reward_vs_baseline:+.1f}% reward\")\n",
    "print(f\"   vs Random: {reward_vs_random:+.1f}% reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41b102fb-2ae3-40e4-be24-97fe15b80b77",
    "_uuid": "4663298b-be04-4b60-8746-ce9912d78e6c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 6. Visualize Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4615a8dd-4d6c-4f33-a5f9-d1cb0f9f941b",
    "_uuid": "33e55f98-5548-4565-ae04-d29b5a09b824",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:42:07.992632Z",
     "iopub.status.busy": "2025-11-08T19:42:07.992144Z",
     "iopub.status.idle": "2025-11-08T19:42:08.015407Z",
     "shell.execute_reply": "2025-11-08T19:42:08.014619Z",
     "shell.execute_reply.started": "2025-11-08T19:42:07.992612Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reward distribution comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "policies_data = {\n",
    "    'LinUCB': policy_eval.baseline_results['linucb']['reward'],\n",
    "    'Weakest-Skill-First': policy_eval.baseline_results['weakest']['reward'],\n",
    "    'Random': policy_eval.baseline_results['random']['reward']\n",
    "}\n",
    "\n",
    "for policy_name, rewards in policies_data.items():\n",
    "    fig.add_trace(go.Box(\n",
    "        y=rewards,\n",
    "        name=policy_name,\n",
    "        boxmean='sd'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Reward Distribution by Policy',\n",
    "    yaxis_title='Reward',\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Cumulative reward over time\n",
    "fig = go.Figure()\n",
    "\n",
    "for policy_name, rewards in policies_data.items():\n",
    "    cumulative_reward = np.cumsum(rewards)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(cumulative_reward))),\n",
    "        y=cumulative_reward,\n",
    "        mode='lines+markers',\n",
    "        name=policy_name\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cumulative Reward Over Sessions',\n",
    "    xaxis_title='Session',\n",
    "    yaxis_title='Cumulative Reward',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a46f9802-2889-45df-b939-f06a60b3a5f3",
    "_uuid": "53249d7a-66ea-404c-9748-9408e5eda4d5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 7. Alignment Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b6a1884-9997-4750-8816-aa1e855fd013",
    "_uuid": "7e5120d5-dcd9-4ae3-8eb7-9f0af6d8db1f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:42:13.145074Z",
     "iopub.status.busy": "2025-11-08T19:42:13.144835Z",
     "iopub.status.idle": "2025-11-08T19:42:13.170221Z",
     "shell.execute_reply": "2025-11-08T19:42:13.169580Z",
     "shell.execute_reply.started": "2025-11-08T19:42:13.145059Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_alignment(session_idx: int, data_df: pd.DataFrame, \n",
    "                   policy_df: pd.DataFrame, llm_features: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Check if policy focus aligns with actual weaknesses.\n",
    "    \n",
    "    Returns alignment analysis for a specific session.\n",
    "    \"\"\"\n",
    "    current_row = data_df.iloc[session_idx]\n",
    "    policy_row = policy_df[policy_df['session_idx'] == session_idx].iloc[0]\n",
    "    \n",
    "    # Get rubric scores\n",
    "    rubric_to_action = {\n",
    "        'clarity and enthusiasm in pitch': 'clarity',\n",
    "        'active listening and objection handling': 'active_listening',\n",
    "        'effective call to action': 'call_to_action',\n",
    "        'friendliness and respectful tone': 'friendliness'\n",
    "    }\n",
    "    \n",
    "    rubric_scores = {\n",
    "        action: current_row[rubric]\n",
    "        for rubric, action in rubric_to_action.items()\n",
    "    }\n",
    "    \n",
    "    # Find weakest rubric skill\n",
    "    weakest_rubric_skill = min(rubric_scores, key=rubric_scores.get)\n",
    "    weakest_rubric_score = rubric_scores[weakest_rubric_skill]\n",
    "    \n",
    "    # Get LLM feature scores\n",
    "    llm_scores = {\n",
    "        feat: current_row[feat]\n",
    "        for feat in llm_features\n",
    "    }\n",
    "    \n",
    "    # Map LLM features to skills (approximate mapping)\n",
    "    llm_to_skill = {\n",
    "        'enthusiasm': 'clarity',\n",
    "        'cta_clarity': 'call_to_action',\n",
    "        'objection_handling': 'active_listening',\n",
    "        'question_ratio': 'active_listening',\n",
    "        'empathy_score': 'friendliness',\n",
    "        'collaborative_score': 'friendliness',\n",
    "        'social_proof': 'friendliness'\n",
    "    }\n",
    "    \n",
    "    # Find LLM-flagged weaknesses (bottom 33rd percentile)\n",
    "    llm_threshold = 0.33\n",
    "    llm_weaknesses = {\n",
    "        llm_to_skill[feat]: score\n",
    "        for feat, score in llm_scores.items()\n",
    "        if score < llm_threshold\n",
    "    }\n",
    "    \n",
    "    # Get policy decision\n",
    "    selected_action = policy_row['selected_action']\n",
    "    \n",
    "    # Check alignment\n",
    "    alignment = {\n",
    "        'session_idx': session_idx,\n",
    "        'selected_action': selected_action,\n",
    "        'weakest_rubric_skill': weakest_rubric_skill,\n",
    "        'weakest_rubric_score': weakest_rubric_score,\n",
    "        'matches_weakest_rubric': selected_action == weakest_rubric_skill,\n",
    "        'llm_weaknesses': llm_weaknesses,\n",
    "        'matches_llm_weakness': selected_action in llm_weaknesses,\n",
    "        'rubric_scores': rubric_scores,\n",
    "        'llm_scores': llm_scores\n",
    "    }\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "# Check alignment for first 3 sessions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALIGNMENT SANITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llm_features = ['objection_handling', 'question_ratio', 'cta_clarity',\n",
    "                'empathy_score', 'collaborative_score', 'social_proof', \n",
    "                'enthusiasm']\n",
    "\n",
    "for session_idx in range(min(3, len(policy_df))):\n",
    "    alignment = check_alignment(session_idx, full_df, policy_df, llm_features)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Session {session_idx + 1}:\")\n",
    "    print(f\"   Selected Action: {alignment['selected_action']}\")\n",
    "    print(f\"   Weakest Rubric Skill: {alignment['weakest_rubric_skill']} (score: {alignment['weakest_rubric_score']:.1f})\")\n",
    "    print(f\"   Matches Weakest Rubric: {'âœ“ YES' if alignment['matches_weakest_rubric'] else 'âœ— NO'}\")\n",
    "    print(f\"   LLM-Flagged Weaknesses: {list(alignment['llm_weaknesses'].keys())}\")\n",
    "    print(f\"   Matches LLM Weakness: {'âœ“ YES' if alignment['matches_llm_weakness'] else 'âœ— NO'}\")\n",
    "    \n",
    "    # Show rubric scores for context\n",
    "    print(f\"\\n   Rubric Scores:\")\n",
    "    for skill, score in sorted(alignment['rubric_scores'].items(), key=lambda x: x[1]):\n",
    "        marker = \"ðŸ‘‰\" if skill == alignment['selected_action'] else \"  \"\n",
    "        print(f\"   {marker} {skill}: {score:.1f}\")\n",
    "\n",
    "# Calculate overall alignment statistics\n",
    "alignment_stats = []\n",
    "for session_idx in range(len(policy_df)):\n",
    "    if session_idx < len(full_df):\n",
    "        alignment = check_alignment(session_idx, full_df, policy_df, llm_features)\n",
    "        alignment_stats.append({\n",
    "            'matches_weakest_rubric': alignment['matches_weakest_rubric'],\n",
    "            'matches_llm_weakness': alignment['matches_llm_weakness']\n",
    "        })\n",
    "\n",
    "alignment_df = pd.DataFrame(alignment_stats)\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Alignment Statistics:\")\n",
    "print(f\"   Matches Weakest Rubric: {alignment_df['matches_weakest_rubric'].mean():.1%}\")\n",
    "print(f\"   Matches LLM Weakness: {alignment_df['matches_llm_weakness'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9fadb83-27ee-43c2-b2a0-d4a9c43be917",
    "_uuid": "5c30e063-2d84-4c33-9753-d3be93780baf",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 8. Export Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c0ef366-21fc-42fa-9003-3e1ab28634be",
    "_uuid": "c54c9433-109a-49d7-9008-e2c66055d670",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-08T19:42:19.005915Z",
     "iopub.status.busy": "2025-11-08T19:42:19.005626Z",
     "iopub.status.idle": "2025-11-08T19:42:19.031386Z",
     "shell.execute_reply": "2025-11-08T19:42:19.030607Z",
     "shell.execute_reply.started": "2025-11-08T19:42:19.005894Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create evaluation directory\n",
    "os.makedirs('data/evaluation_results', exist_ok=True)\n",
    "\n",
    "# Save ablation results\n",
    "ablation_results.to_csv('data/evaluation_results/feature_ablation.csv', index=False)\n",
    "\n",
    "# Save policy comparison\n",
    "policy_comparison.to_csv('data/evaluation_results/policy_comparison.csv', index=False)\n",
    "\n",
    "# Save alignment statistics\n",
    "alignment_df.to_csv('data/evaluation_results/alignment_statistics.csv', index=False)\n",
    "\n",
    "# Create comprehensive evaluation report\n",
    "evaluation_report = {\n",
    "    'feature_usefulness': {\n",
    "        'ablation_table': ablation_results.to_dict('records'),\n",
    "        'summary': {\n",
    "            'llm_feature_r2_improvement_pct': r2_improvement,\n",
    "            'llm_feature_auc_improvement_pct': auc_improvement,\n",
    "            'best_r2': ablation_results['RÂ² (Regression)'].max(),\n",
    "            'best_auc': ablation_results['AUC (Classification)'].max()\n",
    "        }\n",
    "    },\n",
    "    'policy_performance': {\n",
    "        'comparison_table': policy_comparison.to_dict('records'),\n",
    "        'summary': {\n",
    "            'linucb_avg_reward': linucb_reward,\n",
    "            'linucb_vs_baseline_pct': reward_vs_baseline,\n",
    "            'linucb_vs_random_pct': reward_vs_random,\n",
    "            'linucb_positive_delta_pct': policy_comparison.iloc[0]['% Positive Î”overall']\n",
    "        }\n",
    "    },\n",
    "    'alignment_sanity': {\n",
    "        'matches_weakest_rubric_pct': float(alignment_df['matches_weakest_rubric'].mean()),\n",
    "        'matches_llm_weakness_pct': float(alignment_df['matches_llm_weakness'].mean()),\n",
    "        'total_sessions_analyzed': len(alignment_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/evaluation_results/evaluation_report.json', 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Evaluation results saved to 'data/evaluation_results/'\")\n",
    "print(\"   - feature_ablation.csv\")\n",
    "print(\"   - policy_comparison.csv\")\n",
    "print(\"   - alignment_statistics.csv\")\n",
    "print(\"   - evaluation_report.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8661975,
     "sourceId": 13628451,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
